{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":329006,"sourceType":"datasetVersion","datasetId":139630}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports and Helper functions","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport json\nimport pickle\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import DenseNet201, nasnet\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, average_precision_score\nfrom sklearn.preprocessing import label_binarize, LabelEncoder","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Load the dataset\nfile_path = '/kaggle/input/fashion-product-images-dataset/fashion-dataset/styles.csv'\neda_df = pd.read_csv(file_path, on_bad_lines='skip')\neda_df = eda_df[['id', 'gender', 'masterCategory', 'subCategory', 'articleType', 'baseColour', 'season', 'year', 'usage', 'productDisplayName']]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display basic information about the dataset\nprint(\"Dataset Information:\")\nprint(eda_df.info())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(eda_df.describe(include='all'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nFirst few rows of the dataset:\")\nprint(eda_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Number of unique values in each column\nprint(\"\\nNumber of unique values in each column:\")\nprint(eda_df.nunique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the occurrences of each articleType\narticle_counts = eda_df['articleType'].value_counts()\n\n# Total number of classes before preprocessing\ntotal_classes = len(article_counts)\n\nprint(f\"Total number of classes in 'articleType' before preprocessing: {total_classes}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the cumulative percentage\ncumulative_percentage = 100 * article_counts.cumsum() / article_counts.sum()\n\n# Find the number of classes that represent 95% of the data\nclasses_95_percent = (cumulative_percentage <= 95).sum()\n\nprint(f\"Number of classes in 'articleType' representing 95% of the data: {classes_95_percent}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Visualization","metadata":{}},{"cell_type":"code","source":"def create_pie_and_bar_chart(data, title, filename, top_n=20):\n    # Pie chart\n    plt.figure(figsize=(12, 12))  \n    top_categories = data.head(top_n)\n    others = pd.Series({'Others': data.iloc[top_n:].sum()})\n    pie_data = pd.concat([top_categories, others])\n    \n    colors = sns.color_palette(\"husl\", len(pie_data))\n    plt.pie(pie_data.values, labels=pie_data.index, autopct='%1.1f%%', startangle=90, colors=colors)\n    plt.title(f'Top {top_n} Categories', fontsize=28)  \n    plt.axis('equal')\n    \n    # Increase font size for pie chart labels and percentages\n    plt.rcParams['font.size'] = 16  \n    \n    plt.savefig(f'{filename}_pie.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    plt.close()\n\n    # Bar chart\n    plt.figure(figsize=(15, 15))  \n    remaining_categories = data.iloc[top_n:classes_95_percent]\n    remaining_categories = remaining_categories.sort_values(ascending=True)\n    \n    colors = sns.color_palette(\"YlOrBr\", len(remaining_categories))\n    bars = plt.barh(range(len(remaining_categories)), remaining_categories.values, color=colors)\n    plt.yticks(range(len(remaining_categories)), remaining_categories.index, fontsize=16)  \n    plt.xlabel('Count', fontsize=18)  \n    plt.title(f'Remaining Categories up to 95% of Data', fontsize=26)  \n    \n    # Add value labels to the end of each bar\n    for bar in bars:\n        width = bar.get_width()\n        plt.text(width, bar.get_y() + bar.get_height()/2, f'{width}', \n                 ha='left', va='center', fontsize=16)  \n    \n    plt.tight_layout()\n    plt.savefig(f'{filename}_bar.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    plt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create charts for all classes up to 95%\ncreate_pie_and_bar_chart(article_counts, 'Distribution of Article Types (95% of Data)', 'article_types_distribution')\n\nprint(f\"Charts have been saved as 'article_types_distribution_pie.png' and 'article_types_distribution_bar.png'\")\n\n# Optional: Print the classes representing 95% of the data\nclasses = article_counts.index[:classes_95_percent].tolist()\nprint(\"\\nClasses representing 95% of the data:\")\nfor i, class_name in enumerate(classes, 1):\n    print(f\"{i}. {class_name}: {article_counts[class_name]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load and Preprocess Data","metadata":{}},{"cell_type":"code","source":"# Disable warnings\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n\n# Set the style for the plots\nsns.set_palette(\"pastel\")\n\ndef load_and_preprocess_data(data_dir, valid_samples=100):\n    \"\"\"Loads the dataset, preprocesses it, and splits it into train, validation, and test sets.\"\"\"\n    images_folder = os.path.join(data_dir, \"images\")\n    styles_path = os.path.join(data_dir, \"styles.csv\")\n\n    df = pd.read_csv(styles_path, on_bad_lines=\"skip\")\n    df = df[df[\"id\"].apply(lambda x: os.path.isfile(os.path.join(images_folder, str(x) + \".jpg\")))]\n    df[\"image\"] = df[\"id\"].apply(lambda x: os.path.join(images_folder, str(x) + \".jpg\"))\n    df = df[[\"image\", \"articleType\"]]\n\n    # Filter categories with at least valid_samples samples\n    valid_categories = df[\"articleType\"].value_counts()[df[\"articleType\"].value_counts() >= valid_samples].index\n    df = df[df[\"articleType\"].isin(valid_categories)]\n\n    # Split data into train, validation, and test sets\n    train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df[\"articleType\"], random_state=42)\n    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"articleType\"], random_state=42)\n    \n    # Calculate total number of images\n    total_images = len(df)\n\n    # Print the number of samples in each set\n    print(f\"Total number of images after filtering: {total_images}\")\n    print(f\"Number of samples in train set: {len(train_df)}\")\n    print(f\"Number of samples in validation set: {len(val_df)}\")\n    print(f\"Number of samples in test set: {len(test_df)}\")\n    \n    return train_df, val_df, test_df, total_images","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Resize with Padding\ndef resize_with_padding(image, target_height, target_width):\n    \"\"\"Resizes the image to the target size with padding.\"\"\"\n    height, width = tf.shape(image)[0], tf.shape(image)[1]\n\n    if tf.equal(height, 0) or tf.equal(width, 0):\n        return tf.zeros([target_height, target_width, 3], dtype=tf.float32)\n\n    scale = tf.minimum(\n        tf.cast(target_width, tf.float32) / tf.cast(width, tf.float32),\n        tf.cast(target_height, tf.float32) / tf.cast(height, tf.float32),\n    )\n    new_height = tf.cast(tf.cast(height, tf.float32) * scale, tf.int32)\n    new_width = tf.cast(tf.cast(width, tf.float32) * scale, tf.int32)\n    resized_image = tf.image.resize(image, [new_height, new_width])\n    padded_image = tf.image.pad_to_bounding_box(\n        resized_image,\n        (target_height - new_height) // 2,\n        (target_width - new_width) // 2,\n        target_height,\n        target_width,\n    )\n    return padded_image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess Image\ndef preprocess_image(image_path, target_size, preprocess_input_func):\n    \"\"\"Loads, resizes, and preprocesses a single image.\"\"\"\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = resize_with_padding(image, target_size[0], target_size[1])\n    image = preprocess_input_func(image)\n    return image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create Dataset\ndef create_dataset(df, batch_size, target_size, preprocess_input_func, is_training=False):\n    \"\"\"Creates a TensorFlow dataset from the dataframe.\"\"\"\n    image_paths = df[\"image\"].values\n    labels = pd.get_dummies(df[\"articleType\"]).values\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n\n    if is_training:\n        dataset = dataset.shuffle(buffer_size=len(df))\n\n    dataset = dataset.map(\n        lambda image_path, label: (preprocess_image(image_path, target_size, preprocess_input_func), label),\n        num_parallel_calls=tf.data.AUTOTUNE,\n    )\n    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build the Models","metadata":{}},{"cell_type":"code","source":"def build_model(model_name, input_shape, num_classes):\n    \"\"\"Builds and compiles the CNN model.\"\"\"           \n    if model_name == \"ResNet50\":\n        base_model = tf.keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n    elif model_name == \"DenseNet201\":\n        base_model = tf.keras.applications.DenseNet201(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n    else:\n        raise ValueError(f\"Unsupported model: {model_name}\")\n\n    # Make the last 20 layers trainable\n    for layer in base_model.layers[-20:]:\n        layer.trainable = True\n\n    model = tf.keras.Sequential([\n        base_model,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(2048, activation=\"relu\"),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n    ])\n\n    model.compile(loss=\"categorical_crossentropy\",\n                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n                  metrics=[\"accuracy\"])\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Training Function","metadata":{}},{"cell_type":"code","source":"def train_model(model, model_name, train_dataset, val_dataset, epochs=30):\n    \"\"\"Trains the model and returns the training history.\"\"\"\n    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        f\"best_model_{model_name}.keras\",\n        save_best_only=True,\n        monitor=\"val_accuracy\",\n        mode=\"max\",\n        verbose=1\n    )\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True),\n        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.4, patience=3, min_lr=1e-6),\n        model_checkpoint\n    ]\n\n    history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, verbose=1, callbacks=callbacks)\n    return history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Confusion Matrix Plotting Function","metadata":{}},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):\n    plt.figure(figsize=(30, 25))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n    plt.tight_layout()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Macro-Average Precision-Recall Curve Plotting Function","metadata":{}},{"cell_type":"code","source":"def plot_macro_average_prc_curve(true_labels, predicted_probs, class_labels, model_name):\n    plt.figure(figsize=(10, 8))\n    \n    # Binarize the labels\n    true_labels_bin = label_binarize(true_labels, classes=range(len(class_labels)))\n    \n    # Compute Precision-Recall curve and average precision for each class\n    precision = dict()\n    recall = dict()\n    average_precision = dict()\n    for i in range(len(class_labels)):\n        precision[i], recall[i], _ = precision_recall_curve(true_labels_bin[:, i], predicted_probs[:, i])\n        average_precision[i] = average_precision_score(true_labels_bin[:, i], predicted_probs[:, i])\n    \n    # Compute macro-average precision-recall curve\n    precision[\"macro\"], recall[\"macro\"], _ = precision_recall_curve(true_labels_bin.ravel(), predicted_probs.ravel())\n    average_precision[\"macro\"] = average_precision_score(true_labels_bin, predicted_probs, average=\"macro\")\n    \n    # Plot macro-average precision-recall curve\n    plt.plot(recall[\"macro\"], precision[\"macro\"], \n             label=f'Macro-average PRC (AP = {average_precision[\"macro\"]:.2f})',\n             color='navy', linestyle='-', linewidth=2)\n\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title(f'Macro-Average Precision-Recall Curve for {model_name}')\n    plt.legend(loc=\"lower left\")\n    plt.savefig(f'macro_avg_prc_{model_name}.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    plt.close()\n\n    return average_precision[\"macro\"]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Configuration","metadata":{}},{"cell_type":"code","source":"# Configuration\ndata_dir = \"/kaggle/input/fashion-product-images-dataset/fashion-dataset\"\nbatch_size = 128\nepochs = 30\nmodels = [\"ResNet50\", \"DenseNet201\"]\n\nmodel_configs = {\n    \"ResNet50\": {\"target_size\": (224, 224), \"preprocess_input\": tf.keras.applications.resnet50.preprocess_input},\n    \"DenseNet201\": {\"target_size\": (224, 224), \"preprocess_input\": tf.keras.applications.densenet.preprocess_input}\n}\n\n# Load and preprocess data\ntrain_df, val_df, test_df, total_images = load_and_preprocess_data(data_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Split Distribution","metadata":{}},{"cell_type":"code","source":"# Create a pie chart\nplt.figure(figsize=(14, 7))  # Increased figure size\nsizes = [len(train_df), len(val_df), len(test_df)]\nlabels = ['Train', 'Validation', 'Test']\ncolors = sns.color_palette(\"pastel\")[0:3]\n\nplt.pie(sizes, labels=labels, colors=colors, autopct=lambda pct: f'{pct:.1f}%\\n({int(pct/100.*sum(sizes))})', \n        startangle=90, textprops={'fontsize': 16}, explode=(0.1, 0, 0))\nplt.title(f'Distribution of Images in Train, Validation, and Test Sets\\nTotal Images: {total_images}', fontsize=22)\nplt.axis('equal')\n\n# Add a much larger legend\nplt.legend(title=\"Datasets\", loc=\"center left\", bbox_to_anchor=(1, 0.5), fontsize=18, title_fontsize=20, \n           labelspacing=1.5, handlelength=3, handletextpad=1.5)\n\n# Adjust layout to make room for the legend\nplt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust the right margin to accommodate the legend\n\n# Save the pie chart\nplt.savefig('data_split_distribution.png', dpi=300, bbox_inches='tight')\nplt.show()\nplt.close()\n\nprint(\"Pie chart has been saved as 'data_split_distribution.png'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save test dataset\ntest_df.to_csv('test_dataset.csv', index=False)\n\n# Save model configurations\nwith open('model_configs.json', 'w') as f:\n    json.dump({k: {**v, \"preprocess_input\": v[\"preprocess_input\"].__name__} for k, v in model_configs.items()}, f)\n\n# Save class labels\nclass_labels = np.unique(train_df[\"articleType\"])\nnp.save('class_labels.npy', class_labels)\n\n# Get a random sample of 9 images\nfor model_name, config in model_configs.items():\n    print(f\"\\nDisplaying resized images for {model_name}:\")\n\n    # Get a random sample of 9 images\n    sample_df = train_df.sample(n=9)\n\n    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n    for i, (_, row) in enumerate(sample_df.iterrows()):\n        img = preprocess_image(row['image'], config['target_size'], config['preprocess_input'])\n        img = img.numpy()\n\n        # Denormalize the image for display\n        img = (img - np.min(img)) / (np.max(img) - np.min(img))\n\n        ax = axes[i // 3, i % 3]\n        ax.imshow(img)\n        ax.set_title(row['articleType'])\n        ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# Detect and connect to a TPU if available\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(\"Running on TPU \", tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError:\n    print(\"TPU is not available, using default strategy\")\n    strategy = tf.distribute.get_strategy()\n\n# Train and evaluate models\nhistories = {}\nfor model_name in models:\n    print(f\"Training {model_name} model...\")\n\n    config = model_configs[model_name]\n    train_dataset = create_dataset(\n        train_df,\n        batch_size,\n        config[\"target_size\"],\n        config[\"preprocess_input\"],\n        is_training=True,\n    )\n    val_dataset = create_dataset(\n        val_df, batch_size, config[\"target_size\"], config[\"preprocess_input\"]\n    )\n\n    with strategy.scope():\n        model = build_model(model_name, (*config[\"target_size\"], 3), len(class_labels))\n        history = train_model(model, model_name, train_dataset, val_dataset, epochs)\n        histories[model_name] = history\n\n    # Save training history\n    with open(f'history_{model_name}.pkl', 'wb') as f:\n        pickle.dump(history.history, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluation\ntest_results = {}\npredicted_labels = {}\npredicted_probabilities = {}\n\nfor model_name in models:\n    print(f\"Evaluating {model_name} model...\")\n\n    config = model_configs[model_name]\n    test_dataset = create_dataset(\n        test_df, batch_size, config[\"target_size\"], config[\"preprocess_input\"]\n    )\n\n    # Load the best model\n    model = tf.keras.models.load_model(f\"best_model_{model_name}.keras\")\n    test_loss, test_accuracy = model.evaluate(test_dataset, verbose=1)\n    test_results[model_name] = {'loss': test_loss, 'accuracy': test_accuracy}\n\n    predictions = model.predict(test_dataset)\n    predicted_classes = np.argmax(predictions, axis=1)\n    predicted_labels[model_name] = predicted_classes\n    predicted_probabilities[model_name] = predictions\n\n    print(f\"{model_name} Test Loss: {test_loss:.4f}\")\n    print(f\"{model_name} Test Accuracy: {test_accuracy * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# True labels\ntrue_labels = np.argmax(pd.get_dummies(test_df[\"articleType\"]).values, axis=1)\n\n# Print classification report for each model\nfor model_name in models:\n    print(f\"Classification Report for {model_name}:\")\n    print(classification_report(true_labels, predicted_labels[model_name], target_names=class_labels))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot confusion matrix for each model\nfor model_name in models:\n    cm = confusion_matrix(true_labels, predicted_labels[model_name])\n    plt.figure(figsize=(30, 25))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n    plt.title(f'Confusion Matrix for {model_name}')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.xticks(rotation=90)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.savefig(f'confusion_matrix_{model_name}.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    plt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare model performance\nprint(\"Model Performance Comparison:\")\nfor model_name, result in test_results.items():\n    print(f\"{model_name}: Test Loss = {result['loss']:.4f}, Test Accuracy = {result['accuracy'] * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot Precision-Recall curve and calculate AUPRC for each model\nmacro_auprcs = {}\nfor model_name in models:\n    macro_auprcs[model_name] = plot_macro_average_prc_curve(true_labels, predicted_probabilities[model_name], class_labels, model_name)\n\n# Print macro-average AUPRC scores for all models\nprint(\"\\nMacro-Average AUPRC Scores:\")\nfor model_name, auprc_score in macro_auprcs.items():\n    print(f\"{model_name}: {auprc_score:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training history for all models\nfor model_name, history in histories.items():\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='validation')\n    plt.title(f'{model_name} - Accuracy')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='train')\n    plt.plot(history.history['val_loss'], label='validation')\n    plt.title(f'{model_name} - Loss')\n    plt.legend()\n\n    plt.savefig(f'training_history_{model_name}.png')\n    plt.show()\n    plt.close()\n\nprint(\"Evaluation completed for all models.\")\nprint(f\"Class labels: {class_labels}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Best Performing Model","metadata":{}},{"cell_type":"code","source":"# Identify the best performing model\nbest_model = max(test_results, key=lambda x: test_results[x]['accuracy'])\nprint(f\"\\nBest performing model: {best_model}\")\nprint(f\"Best model accuracy: {test_results[best_model]['accuracy'] * 100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Misclassified Pairs","metadata":{}},{"cell_type":"code","source":"# Additional analysis for the best model\nbest_model_predictions = predicted_labels[best_model]\n\n# Top misclassified classes\nmisclassified = true_labels != best_model_predictions\nmisclassified_df = pd.DataFrame({\n    'True': [class_labels[i] for i in true_labels[misclassified]],\n    'Predicted': [class_labels[i] for i in best_model_predictions[misclassified]]\n})\ntop_misclassified = misclassified_df.groupby(['True', 'Predicted']).size().sort_values(ascending=False).head(10)\n\nprint(\"\\nTop 10 misclassified pairs:\")\nprint(top_misclassified)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save top misclassified pairs\ntop_misclassified.to_csv('top_misclassified.csv')\n\n# Class-wise accuracy\nclass_accuracy = classification_report(true_labels, best_model_predictions, target_names=class_labels, output_dict=True)\nclass_accuracy_df = pd.DataFrame(class_accuracy).transpose()\nclass_accuracy_df = class_accuracy_df.sort_values('f1-score', ascending=False)\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=class_accuracy_df.index, y=class_accuracy_df['f1-score'])\nplt.title('F1-score by Class for Best Model')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.savefig('f1_score_by_class.png')\nplt.show()\nplt.close()\n\nprint(\"\\nAnalysis completed. All results and visualizations have been saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Comparison","metadata":{}},{"cell_type":"code","source":"# Dictionary to store the training histories\nhistories = {}\n\n# Load the training histories from saved pickle files\nfor model_name in models:\n    file_path = f'history_{model_name}.pkl'  # Adjust path if saved elsewhere (e.g., '/kaggle/working/history_{model_name}.pkl')\n    try:\n        with open(file_path, 'rb') as f:\n            histories[model_name] = pickle.load(f)\n        print(f\"Loaded history for {model_name}\")\n    except FileNotFoundError:\n        print(f\"Error: History file for {model_name} not found at {file_path}\")\n        continue\n\n# Extract the final validation accuracy (converted to percentage)\nval_accuracies = {model: histories[model]['val_accuracy'][-1] * 100 for model in histories}\n\n# Sort models by validation accuracy\nsorted_models = sorted(val_accuracies.items(), key=lambda x: x[1], reverse=True)\n\n# Prepare data for plotting\nmodel_names = [model for model, _ in sorted_models]\naccuracies = [acc for _, acc in sorted_models]\n\n# Create the horizontal bar plot\nplt.figure(figsize=(10, 6))\nsns.barplot(x=accuracies, y=model_names, palette=\"viridis\")\nplt.title('Model Comparison - Validation Accuracy (%)')\nplt.xlabel('Validation Accuracy (%)')\nplt.ylabel('Model')\nplt.xlim(0, 100)  # Accuracy in percentage (0 to 100)\nfor i, v in enumerate(accuracies):\n    plt.text(v + 1, i, f'{v:.1f}%', va='center')  # Add percentage labels\nplt.grid(axis='x')\n\n# Save the plot to a file\nplt.savefig('/kaggle/working/model_comparison.png', dpi=300)  # Adjust path as needed\nplt.show()\nplt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}